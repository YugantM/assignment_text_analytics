{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import textcleaner as tc\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get files from https://nlp.stanford.edu/software/stanford-ner-2018-10-16.zip\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reference https://pypi.org/project/rake-nltk/\n",
    "from rake_nltk import Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this list will be used to create a data frame\n",
    "fframe = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corp = tc.corpus('corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up name entity recognition model\n",
    "st = StanfordNERTagger('stanford-ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "                       'stanford-ner/stanford-ner.jar',\n",
    "                        encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting up Rake instance\n",
    "# Uses stopwords for english from NLTK, and all puntuation characters.\n",
    "r = Rake() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating highlights from all the documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ANACONDA\\lib\\site-packages\\pandas\\core\\indexes\\api.py:77: RuntimeWarning: '<' not supported between instances of 'str' and 'int', sort order is undefined for incomparable objects\n",
      "  result = result.union(other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################done\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating highlights from all the documents...\")\n",
    "\n",
    "sorted_file_list = [str(x)+\".txt\" for x in sorted([int(each.split('.txt')[0]) for each in corp.files])]\n",
    "for file in sorted_file_list:\n",
    "    doc = tc.document(corp.root+\"/\"+str(file))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # summurizing the document to make the text data dense\n",
    "    text = summarize(\" \".join(doc.data))\n",
    "    \n",
    "    #tokenized_text = word_tokenize(text)\n",
    "    #classified_text = st.tag(tokenized_text)\n",
    "    \n",
    "    #features = list(set([each[1] for each in classified_text]))\n",
    "    \n",
    "    #creates dictionary out of list of tuples \n",
    "    #result = {}\n",
    "    #for fea in features:\n",
    "    #    result[fea]=[]\n",
    "        \n",
    "    #for each in classified_text:\n",
    "    #    result[each[1]].append(each[0])\n",
    "     \n",
    "    '''\n",
    "    highlights = []\n",
    "    for fea in features:\n",
    "        # if the tagged featrue is not others\n",
    "        if fea!=\"O\":\n",
    "            # look for the words in the given category\n",
    "            for word in result[fea]:\n",
    "\n",
    "                highlights.append([sent for sent in text.split('\\n') if word in sent])\n",
    "\n",
    "    final_highlights = list(set([\"\".join(x) for x in highlights]))           '''    \n",
    "    \n",
    "    #r.extract_keywords_from_text(\"\".join(list(set([\"\".join(x) for x in highlights]))))\n",
    "    r.extract_keywords_from_text(text)\n",
    "\n",
    "    phrase = tc.remove_symbols(r.get_ranked_phrases()[0])[0] # To get keyword phrases ranked highest to lowest.\n",
    "    \n",
    "    \n",
    "    # the highest ranked phrase is used in the given paragraph from the document\n",
    "    for sent in text.split(\"\\n\"):       \n",
    "        if phrase in str(tc.document(sent).lower_all().remove_symbols().data):\n",
    "            fframe.append([file,phrase,sent])\n",
    "            print('#',end='')\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fframe = pd.DataFrame(fframe,columns=[\"doc\",\"top phrase\",\"highlight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>top phrase</th>\n",
       "      <th>highlight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.txt</td>\n",
       "      <td>market leadership despite aggressive competitive entry</td>\n",
       "      <td>Amazon.com passed many milestones in 1997: by year-end, we had served more than 1.5 million customers, yielding 838% revenue growth to $147.8 million, and extended our market leadership despite aggressive competitive entry.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>1 billion revenue run rate</td>\n",
       "      <td>We’ve served a cumulative 6.2 million customers, exited 1998 with a $1 billion revenue run rate, launched music, video, and gift stores in the U.S., opened shop in the U.K. and Germany, and, just recently, launched Amazon.com Auctions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.txt</td>\n",
       "      <td>1 billion revenue run rate</td>\n",
       "      <td>We’ve served a cumulative 6.2 million customers, exited 1998 with a $1 billion revenue run rate, launched music, video, and gift stores in the U.S., opened shop in the U.K. and Germany, and, just recently, launched Amazon.com Auctions.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.txt</td>\n",
       "      <td>books business represented 100 percent</td>\n",
       "      <td>Only two years ago, Amazon.com's U.S. Books business represented 100 percent of our sales.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.txt</td>\n",
       "      <td>much bandwidth per customer 5 years</td>\n",
       "      <td>Given that last doubling rate, Amazon.com will be able to use 60 times as much bandwidth per customer 5 years from now while holding our bandwidth cost per customer constant.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.txt</td>\n",
       "      <td>features like instant order update</td>\n",
       "      <td>We’ve improved convenience with features like Instant Order Update, which warns you if you’re about to buy the same item twice (people are busy--they forget that they’ve already bought it!).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.txt</td>\n",
       "      <td>year free super saver shipping</td>\n",
       "      <td>We’ve been doing so broadly across product categories, from books to electronics, and we’ve eliminated shipping fees with our 365 day-per-year Free Super Saver Shipping on orders over $25.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.txt</td>\n",
       "      <td>helping customers make better purchase decisions ultimately pays</td>\n",
       "      <td>Though negative reviews cost us some sales in the short term, helping customers make better purchase decisions ultimately pays off for the company.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.txt</td>\n",
       "      <td>transportation business generates cumulative negative free cash flow</td>\n",
       "      <td>Over the same four years, the transportation business generates cumulative negative free cash flow of $530 million.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.txt</td>\n",
       "      <td>new customer set software developers</td>\n",
       "      <td>With AWS, we’re building a new business focused on a new customer set … software developers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.txt</td>\n",
       "      <td>electronic book signings thirteen years later</td>\n",
       "      <td>I was asked about a particular feature dozens of times: “How are you going to do electronic book signings?” Thirteen years later, we still haven’t figured that one out!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.txt</td>\n",
       "      <td>primary financial goal remains maximizing long</td>\n",
       "      <td>Our primary financial goal remains maximizing long-term free cash flow and doing so with high rates of return on invested capital.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.txt</td>\n",
       "      <td>added 21 new product categories around</td>\n",
       "      <td>We added 21 new product categories around the world in 2009, including Automotive in Japan, Baby in France, and Shoes and Apparel in China.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.txt</td>\n",
       "      <td>key data services store many petabytes</td>\n",
       "      <td>Many years ago, Amazon’s requirements reached a point where many of our systems could no longer be served by any commercial solution: our key data services store many petabytes of data and handle millions of requests per second.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14.txt</td>\n",
       "      <td>multiple kindle best sellers including abducted</td>\n",
       "      <td>The publishing world is changing fast, and I plan to enjoy every minute of the ride.” Theresa Ragan is the KDP author of multiple Kindle best sellers including Abducted.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15.txt</td>\n",
       "      <td>reduced aws prices 27 times since launching 7 years ago</td>\n",
       "      <td>We’ve reduced AWS prices 27 times since launching 7 years ago, added enterprise service support enhancements, and created innovative tools to help customers be more efficient.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16.txt</td>\n",
       "      <td>reinventing normal creating inventions</td>\n",
       "      <td>Nothing gives us more pleasure at Amazon than “reinventing normal” – creating inventions that customers love and resetting their expectations for what normal should be.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17.txt</td>\n",
       "      <td>many customers like conde nast</td>\n",
       "      <td>And many customers like Conde´ Nast, Kellogg’s, and News Corp are migrating legacy critical applications and, in some cases, entire datacenters to AWS.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18.txt</td>\n",
       "      <td>every size across nearly every industry companies like pinterest</td>\n",
       "      <td>AWS started with developers and startups, and now is used by more than a million customers from organizations of every size across nearly every industry – companies like Pinterest, Airbnb, GE, Enel, Capital One, Intuit, Johnson &amp; Johnson, Philips, Hess, Adobe, McDonald’s, and Time Inc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19.txt</td>\n",
       "      <td>established company might harvest day 2</td>\n",
       "      <td>An established company might harvest Day 2 for decades, but the final result would still come.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc  \\\n",
       "0   0.txt    \n",
       "1   1.txt    \n",
       "2   2.txt    \n",
       "3   3.txt    \n",
       "4   4.txt    \n",
       "5   5.txt    \n",
       "6   6.txt    \n",
       "7   7.txt    \n",
       "8   8.txt    \n",
       "9   9.txt    \n",
       "10  10.txt   \n",
       "11  11.txt   \n",
       "12  12.txt   \n",
       "13  13.txt   \n",
       "14  14.txt   \n",
       "15  15.txt   \n",
       "16  16.txt   \n",
       "17  17.txt   \n",
       "18  18.txt   \n",
       "19  19.txt   \n",
       "\n",
       "                                                              top phrase  \\\n",
       "0   market leadership despite aggressive competitive entry                 \n",
       "1   1 billion revenue run rate                                             \n",
       "2   1 billion revenue run rate                                             \n",
       "3   books business represented 100 percent                                 \n",
       "4   much bandwidth per customer 5 years                                    \n",
       "5   features like instant order update                                     \n",
       "6   year free super saver shipping                                         \n",
       "7   helping customers make better purchase decisions ultimately pays       \n",
       "8   transportation business generates cumulative negative free cash flow   \n",
       "9   new customer set software developers                                   \n",
       "10  electronic book signings thirteen years later                          \n",
       "11  primary financial goal remains maximizing long                         \n",
       "12  added 21 new product categories around                                 \n",
       "13  key data services store many petabytes                                 \n",
       "14  multiple kindle best sellers including abducted                        \n",
       "15  reduced aws prices 27 times since launching 7 years ago                \n",
       "16   reinventing normal creating inventions                                \n",
       "17  many customers like conde nast                                         \n",
       "18  every size across nearly every industry companies like pinterest       \n",
       "19  established company might harvest day 2                                \n",
       "\n",
       "                                                                                                                                                                                                                                                                                         highlight  \n",
       "0   Amazon.com passed many milestones in 1997: by year-end, we had served more than 1.5 million customers, yielding 838% revenue growth to $147.8 million, and extended our market leadership despite aggressive competitive entry.                                                                 \n",
       "1   We’ve served a cumulative 6.2 million customers, exited 1998 with a $1 billion revenue run rate, launched music, video, and gift stores in the U.S., opened shop in the U.K. and Germany, and, just recently, launched Amazon.com Auctions.                                                     \n",
       "2   We’ve served a cumulative 6.2 million customers, exited 1998 with a $1 billion revenue run rate, launched music, video, and gift stores in the U.S., opened shop in the U.K. and Germany, and, just recently, launched Amazon.com Auctions.                                                     \n",
       "3   Only two years ago, Amazon.com's U.S. Books business represented 100 percent of our sales.                                                                                                                                                                                                      \n",
       "4   Given that last doubling rate, Amazon.com will be able to use 60 times as much bandwidth per customer 5 years from now while holding our bandwidth cost per customer constant.                                                                                                                  \n",
       "5   We’ve improved convenience with features like Instant Order Update, which warns you if you’re about to buy the same item twice (people are busy--they forget that they’ve already bought it!).                                                                                                  \n",
       "6   We’ve been doing so broadly across product categories, from books to electronics, and we’ve eliminated shipping fees with our 365 day-per-year Free Super Saver Shipping on orders over $25.                                                                                                    \n",
       "7   Though negative reviews cost us some sales in the short term, helping customers make better purchase decisions ultimately pays off for the company.                                                                                                                                             \n",
       "8   Over the same four years, the transportation business generates cumulative negative free cash flow of $530 million.                                                                                                                                                                             \n",
       "9   With AWS, we’re building a new business focused on a new customer set … software developers.                                                                                                                                                                                                    \n",
       "10  I was asked about a particular feature dozens of times: “How are you going to do electronic book signings?” Thirteen years later, we still haven’t figured that one out!                                                                                                                        \n",
       "11  Our primary financial goal remains maximizing long-term free cash flow and doing so with high rates of return on invested capital.                                                                                                                                                              \n",
       "12  We added 21 new product categories around the world in 2009, including Automotive in Japan, Baby in France, and Shoes and Apparel in China.                                                                                                                                                     \n",
       "13  Many years ago, Amazon’s requirements reached a point where many of our systems could no longer be served by any commercial solution: our key data services store many petabytes of data and handle millions of requests per second.                                                            \n",
       "14  The publishing world is changing fast, and I plan to enjoy every minute of the ride.” Theresa Ragan is the KDP author of multiple Kindle best sellers including Abducted.                                                                                                                       \n",
       "15  We’ve reduced AWS prices 27 times since launching 7 years ago, added enterprise service support enhancements, and created innovative tools to help customers be more efficient.                                                                                                                 \n",
       "16  Nothing gives us more pleasure at Amazon than “reinventing normal” – creating inventions that customers love and resetting their expectations for what normal should be.                                                                                                                        \n",
       "17  And many customers like Conde´ Nast, Kellogg’s, and News Corp are migrating legacy critical applications and, in some cases, entire datacenters to AWS.                                                                                                                                         \n",
       "18  AWS started with developers and startups, and now is used by more than a million customers from organizations of every size across nearly every industry – companies like Pinterest, Airbnb, GE, Enel, Capital One, Intuit, Johnson & Johnson, Philips, Hess, Adobe, McDonald’s, and Time Inc.  \n",
       "19  An established company might harvest Day 2 for decades, but the final result would still come.                                                                                                                                                                                                  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for word wrapping\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "fframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagged entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_file_list = ['corpus/'+str(x)+\".txt\" for x in sorted([int(each.split('.txt')[0]) for each in corp.files])]\n",
    "\n",
    "result_list = []\n",
    "#change list slice in the loop for results of more documents\n",
    "for x in sorted_file_list[:1]:\n",
    "    doc = tc.document(x)\n",
    "\n",
    "    temp = []\n",
    "    temp.append(x)\n",
    "    \n",
    "    text = \" \".join(doc.lower_all().data)\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "    \n",
    "    features = list(set([each[1] for each in classified_text]))\n",
    "    \n",
    "    \n",
    "    #creates dictionary out of list of tuples \n",
    "    result = {}\n",
    "    for fea in features:\n",
    "        if fea!=\"O\":\n",
    "            temp.append(\",\".join([each[0] for each in classified_text if each[1]==fea]))\n",
    "      \n",
    "    result_list.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>DATE</th>\n",
       "      <th>MONEY</th>\n",
       "      <th>PERCENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>corpus/1.txt</td>\n",
       "      <td>amazon.com,amazon.com,netscape,geocities,amazon.com,amazon.com</td>\n",
       "      <td>delaware</td>\n",
       "      <td>jeffrey,p.,bezos,jeffrey,p.,bezos</td>\n",
       "      <td>1997,year-end,1997,1997,1997,1996,the,fourth,quarter,of,1996,1997.,1997,november,1997,1998,1997</td>\n",
       "      <td>$,147.8,million,$,15.7,million,$,147.8,million,$,125,million,$,75,million</td>\n",
       "      <td>838,%,838,%,738,%,46,%,58,%,70,%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc  \\\n",
       "0  corpus/1.txt   \n",
       "\n",
       "                                                     ORGANIZATION  LOCATION  \\\n",
       "0  amazon.com,amazon.com,netscape,geocities,amazon.com,amazon.com  delaware   \n",
       "\n",
       "                              PERSON  \\\n",
       "0  jeffrey,p.,bezos,jeffrey,p.,bezos   \n",
       "\n",
       "                                                                                              DATE  \\\n",
       "0  1997,year-end,1997,1997,1997,1996,the,fourth,quarter,of,1996,1997.,1997,november,1997,1998,1997   \n",
       "\n",
       "                                                                       MONEY  \\\n",
       "0  $,147.8,million,$,15.7,million,$,147.8,million,$,125,million,$,75,million   \n",
       "\n",
       "                            PERCENT  \n",
       "0  838,%,838,%,738,%,46,%,58,%,70,%  "
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.remove('O')\n",
    "pd.DataFrame(result_list,columns=['doc']+features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this module needs updates not working properly\n",
    "\n",
    "import nltk,operator,pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sorted_file_list = [\"corpus/\"+str(x)+\".txt\" for x in sorted([int(each.split('.txt')[0]) for each in corp.files])]\n",
    "\n",
    "inpu=[\" \".join(tc.document(file).main_cleaner().data) for file in sorted_file_list]\n",
    "\n",
    "#no of documents     \n",
    "Nd = len(inpu) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this function finds Term Frequency\n",
    "def TF(word,doc):\n",
    "    doc = \" \".join([\" \".join(each) for each in tc.document(doc).words])\n",
    "    score=doc.count(word)/len(doc.split(' '))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def IDF(word,corpus):\n",
    "    count = 1\n",
    "    for file in corpus:\n",
    "        if word in file:\n",
    "            count+=1\n",
    "    return log(Nd/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(word,sent,file):\n",
    "    score = TF(word,sent)*IDF(word,file)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_score_matrix=[]\n",
    "\n",
    "# clear the slice from the list for \n",
    "for each in inpu[2:4]:\n",
    "\n",
    "    temp_score_matrix = []\n",
    "    unique_words = list(set(each.split(\" \")))\n",
    "    for word in unique_words:\n",
    "        temp_score_matrix.append([word,tfidf(word,each,inpu)])\n",
    "        \n",
    "    temp_score_matrix = pd.DataFrame(score_matrix,columns=['word','score'])\n",
    "    temp = temp_score_matrix.sort_values(['score'],ascending=[0]).head(5)[['word']].values.tolist()\n",
    "    main_score_matrix.append([file,str(\" \".join([\", \".join(each) for each in temp]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>important words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>investment leadership solidify yearend aggressive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.txt</td>\n",
       "      <td>investment leadership solidify yearend aggressive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc                                    important words\n",
       "0  1.txt  investment leadership solidify yearend aggressive\n",
       "1  1.txt  investment leadership solidify yearend aggressive"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_score_matrix = pd.DataFrame(main_score_matrix,columns=['doc','important words'])\n",
    "main_score_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shareholder first year journey yielded amazing result weve served million customer country built lea\n",
      "shareholder ouch brutal year many capital market certainly amazoncom shareholder writing share wrote\n"
     ]
    }
   ],
   "source": [
    "for each in inpu[2:4]:\n",
    "    print(each[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
